{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import os.path\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense\n",
    "from helpers import resize_to_fit\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import winsound\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T06:09:56.728826Z",
     "start_time": "2020-06-08T06:09:56.661999Z"
    }
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = input(\"INPUT FILE NAME:\")\n",
    "\n",
    "g = 50 \n",
    "\n",
    "tangle=input(\"方格大小(80):\")\n",
    "tangle=int(tangle)\n",
    "\n",
    "mx=input(\"分割數(２０的話是０～２０ ４０是０～４0):\")#分割數是看你要切多少單位 ２０的話是０～２０ ４０是０～４0\n",
    "mx=int(mx)\n",
    "my=mx\n",
    "\n",
    "count=input(\"深度數(6)：\")#6\n",
    "count=int(count)\n",
    "\n",
    "deep=input(\"深度級距(40):\")#深度級距通常你打４０就好\n",
    "deep=int(deep)\n",
    "\n",
    "pixel=input(\"訓練影像大小(切２０份就打２８，切４０就打４８)：\")#影像訓練大小看你如果你切２０份就打２８\n",
    "                            #切４０就打４８，壹定要比分割數大\n",
    "pixel=int(pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(pixel<mx):\n",
    "    pixel=input(\"重新輸入訓練影像大小（需大於分割術）：\")\n",
    "    pixel=int(pixel)\n",
    "mapping=(pixel,pixel) \n",
    "a = 0\n",
    "while(a < int(g)):\n",
    "    img = Image.open(\"D:/Work/Picture/multy/\"+f+\"/\"+str(a)+\".jpg\")\n",
    "    img2 = img\n",
    "    outputFolderx = \"D:/Work/Picture/multy/\" + f + \"/Data/X/\"\n",
    "    outputFoldery = \"D:/Work/Picture/multy/\" + f + \"/Data/Y/\"\n",
    "    outputFolderz = \"D:/Work/Picture/multy/\" + f + \"/Data/Z/\"\n",
    "    outputFolder3 = \"D:/Work/Picture/multy/\"+f+\"/code/\"\n",
    "    if not os.path.exists(outputFolderx):\n",
    "        os.makedirs(outputFolderx)\n",
    "    if not os.path.exists(outputFoldery):\n",
    "        os.makedirs(outputFoldery)\n",
    "    if not os.path.exists(outputFolderz):\n",
    "        os.makedirs(outputFolderz)\n",
    "    if not os.path.exists(outputFolder3):\n",
    "        os.makedirs(outputFolder3)\n",
    "\n",
    "    i = 0\n",
    "    while (i < (mx+1)):\n",
    "        outputFolder4 = \"D:/Work/Picture/multy/\" + f + \"/Data/X/\"+str(i)\n",
    "        outputFolder5 = \"D:/Work/Picture/multy/\" + f + \"/Data/Y/\"+str(i)\n",
    "        if not os.path.exists(outputFolder4):\n",
    "            os.makedirs(outputFolder4)\n",
    "        if not os.path.exists(outputFolder5):\n",
    "            os.makedirs(outputFolder5)\n",
    "        i=i+1\n",
    "    i = 0\n",
    "    while (i < count):\n",
    "        outputFolder6 = \"D:/Work/Picture/multy/\" + f + \"/Data/Z/\"+str(i)\n",
    "        if not os.path.exists(outputFolder6):\n",
    "            os.makedirs(outputFolder6)    \n",
    "        i=i+1\n",
    "    coun = 0\n",
    "    while(coun < count):\n",
    "        countx = 0\n",
    "        county = 0\n",
    "        mx = mx #分割數大小＋1\n",
    "        my = my\n",
    "        dx = (500-deep*coun-tangle)/mx # （500-每次深度的大小*深度（0~7）-方格大小）/分割數\n",
    "        dy = (500-deep*coun-tangle)/mx\n",
    "        labelx = np.zeros(21)\n",
    "        labely = np.zeros(21)\n",
    "        while (countx < (mx+1)):\n",
    "            county = 0\n",
    "            while (county < (my+1)):\n",
    "                filename = str(a)+\"_\" +str(coun+1)+'_down' + str(countx) + '_' + str(county)\n",
    "                area = (((tangle/2)+1) +deep*coun+ countx * dx, ((tangle/2)+1)+deep*coun+ county * dy, ((tangle/2)+1)+countx * dx + 500, ((tangle/2)+1) + county * dy + 500)\n",
    "                #area = ((方格大小/2+1)+40*coun+ countx * dx, (方格大小/2+1)+40*coun+ county * dy, \n",
    "                #(方格大小/2+1)+countx * dx + 500, (方格大小/2+1) + county * dy + 500)\n",
    "                img3 = img2.crop(area)\n",
    "                #img3 = img3.resize((mapping),Image. BICUBIC)\n",
    "                img3.save(outputFolderx+str(countx)+\"/\"+filename+\".jpg\")\n",
    "                img3.save(outputFoldery+str(county)+\"/\"+filename+\".jpg\")\n",
    "                img3.save(outputFolderz+str(coun)+\"/\"+filename+\".jpg\")\n",
    "                county = county + 1\n",
    "            countx = countx + 1\n",
    "        coun= coun + 1\n",
    "        print(dx,dy)\n",
    "        print(area)\n",
    "    a+=1\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_IMAGES_FOLDER_X = \"D:/Work/Picture/multy/\"+f+\"/Data/X/\"\n",
    "MODEL_FILENAME_X = \"D:/Work/Picture/multy/\"+f+\"/code/X_model.hdf5\"\n",
    "MODEL_LABELS_FILENAME_X = \"D:/Work/Picture/multy/\"+f+\"/code/X_model_labels.dat\"\n",
    "LETTER_IMAGES_FOLDER_Y = \"D:/Work/Picture/multy/\"+f+\"/Data/Y/\"\n",
    "MODEL_FILENAME_Y = \"D:/Work/Picture/multy/\"+f+\"/code/Y_model.hdf5\"\n",
    "MODEL_LABELS_FILENAME_Y = \"D:/Work/Picture/multy/\"+f+\"/code/Y_model_labels.dat\"\n",
    "LETTER_IMAGES_FOLDER_Z = \"D:/Work/Picture/multy/\"+f+\"/Data/Z/\"\n",
    "MODEL_FILENAME_Z = \"D:/Work/Picture/multy/\"+f+\"/code/Z_model.hdf5\"\n",
    "MODEL_LABELS_FILENAME_Z = \"D:/Work/Picture/multy/\"+f+\"/code/Z_model_labels.dat\"\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images(LETTER_IMAGES_FOLDER_X):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "\n",
    "    image = resize_to_fit(image, pixel,pixel)\n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "    \n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "np.save(\"X\",data)\n",
    "labels = np.array(labels)\n",
    "np.save(\"X_labels\",labels)\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images(LETTER_IMAGES_FOLDER_Y):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "\n",
    "    image = resize_to_fit(image, pixel,pixel)\n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1] (this improves training)\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "np.save(\"Y\",data)\n",
    "labels = np.array(labels)     \n",
    "np.save(\"Y_labels\",labels)\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "for image_file in paths.list_images(LETTER_IMAGES_FOLDER_Z):\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(image_file)\n",
    "\n",
    "    image = resize_to_fit(image, pixel,pixel)\n",
    "\n",
    "    # Add a third channel dimension to the image to make Keras happy  \n",
    "\n",
    "    # Grab the name of the letter based on the folder it was in\n",
    "    label = image_file.split(os.path.sep)[-2]\n",
    "\n",
    "    # Add the letter image and it's label to our training data\n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1] (this improves training)\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "np.save(\"Z\",data)\n",
    "labels = np.array(labels)\n",
    "np.save(\"Z_labels\",labels)\n",
    "print(\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "# loop over the input images\n",
    "data=np.load(\"X.npy\")\n",
    "labels = np.load(\"X_labels.npy\")\n",
    "\n",
    "# Split the training data into separate train and test sets\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Convert the labels (letters) into one-hot encodings that Keras can work with\n",
    "lb = LabelBinarizer().fit(Y_train)\n",
    "Y_train = lb.transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n",
    "\n",
    "# Save the mapping from labels to one-hot encodings.\n",
    "# We'll need this later when we use the model to decode what it's predictions mean\n",
    "with open(MODEL_LABELS_FILENAME_X, \"wb\") as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "# Build the neural network!\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer with max pooling\n",
    "model.add(Conv2D(20, (15, 15), padding=\"same\", input_shape=(pixel,pixel, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3)))\n",
    "\n",
    "# Second convolutional layer with max pooling\n",
    "model.add(Conv2D(10, (8, 8), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# third convolutional layer with max pooling\n",
    "model.add(Conv2D(5, (4, 4), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "# Hidden layer with 500 nodes\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# Output layer with 32 nodes (one for each possible letter/number we predict)\n",
    "model.add(Dense((mx+1), activation=\"softmax\"))\n",
    "\n",
    "# Ask Keras to build the TensorFlow model behind the scenes\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "\n",
    "# Save the trained model to disk\n",
    "model.save(MODEL_FILENAME_X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "data=np.load(\"Y.npy\")\n",
    "labels = np.load(\"Y_labels.npy\")\n",
    "\n",
    "# Split the training data into separate train and test sets\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Convert the labels (letters) into one-hot encodings that Keras can work with\n",
    "lb = LabelBinarizer().fit(Y_train)\n",
    "Y_train = lb.transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n",
    "\n",
    "# Save the mapping from labels to one-hot encodings.\n",
    "# We'll need this later when we use the model to decode what it's predictions mean\n",
    "with open(MODEL_LABELS_FILENAME_Y, \"wb\") as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "# Build the neural network!\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer with max pooling\n",
    "model.add(Conv2D(20, (15, 15), padding=\"same\", input_shape=(pixel,pixel, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3)))\n",
    "\n",
    "# Second convolutional layer with max pooling\n",
    "model.add(Conv2D(10, (8, 8), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# third convolutional layer with max pooling\n",
    "model.add(Conv2D(5, (4, 4), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "# Hidden layer with 500 nodes\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# Output layer with 32 nodes (one for each possible letter/number we predict)\n",
    "model.add(Dense((mx+1), activation=\"softmax\"))\n",
    "\n",
    "# Ask Keras to build the TensorFlow model behind the scenes\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "\n",
    "# Save the trained model to disk\n",
    "model.save(MODEL_FILENAME_Y)\n",
    "\n",
    "\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# loop over the input images\n",
    "data=np.load(\"Z.npy\")\n",
    "labels = np.load(\"Z_labels.npy\")\n",
    "\n",
    "# Split the training data into separate train and test sets\n",
    "(X_train, X_test, Y_train, Y_test) = train_test_split(data, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Convert the labels (letters) into one-hot encodings that Keras can work with\n",
    "lb = LabelBinarizer().fit(Y_train)\n",
    "Y_train = lb.transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n",
    "\n",
    "# Save the mapping from labels to one-hot encodings.\n",
    "# We'll need this later when we use the model to decode what it's predictions mean\n",
    "with open(MODEL_LABELS_FILENAME_Z, \"wb\") as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "# Build the neural network!\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer with max pooling\n",
    "model.add(Conv2D(20, (15, 15), padding=\"same\", input_shape=(pixel,pixel, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3)))\n",
    "\n",
    "# Second convolutional layer with max pooling\n",
    "model.add(Conv2D(10, (8, 8), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# third convolutional layer with max pooling\n",
    "model.add(Conv2D(5, (4, 4), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "\n",
    "# Hidden layer with 500 nodes\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "\n",
    "# Output layer with 32 nodes (one for each possible letter/number we predict)\n",
    "model.add(Dense(count, activation=\"softmax\"))\n",
    "\n",
    "# Ask Keras to build the TensorFlow model behind the scenes\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the neural network\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "\n",
    "# Save the trained model to disk\n",
    "model.save(MODEL_FILENAME_Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
